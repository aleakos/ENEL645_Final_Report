{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE3Q_1TyJqOl"
   },
   "source": [
    "# ENEL Group 5 - Final Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypEiP1nDJqOr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pylab as plt\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO7vlSA5E6VX",
    "outputId": "e3ae1c23-0bad-4335-f73b-b7700c76023a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-23 17:20:17--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: ‘helper_functions.py’\n",
      "\n",
      "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-03-23 17:20:17 (75.4 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get helper_functions.py script from course GitHub\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py \n",
    "\n",
    "# Import helper functions we're going to use\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyXUw7wrC4Kd"
   },
   "source": [
    "### Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rg6p-kDsCS-B"
   },
   "outputs": [],
   "source": [
    "# Define methods to scale images and combine 2d array images.\n",
    "def scale_image(image, scale_type = 'min_max'):\n",
    "    \n",
    "    if scale_type == 'min_max':\n",
    "        img_min = image.min()\n",
    "        img_max = image.max()\n",
    "        \n",
    "        scaled_img = 255 * (image - img_min) / (img_max - img_min)\n",
    "        \n",
    "    return scaled_img\n",
    "\n",
    "def combine_channels(img1, img2, img3):\n",
    "    full_list = []\n",
    "    \n",
    "    for ii in range(img1.shape[0]):\n",
    "        lst = []\n",
    "        \n",
    "        lst.append(scale_image(img1[ii])) #CT\n",
    "        lst.append(scale_image(img2[ii])) #Dose\n",
    "        lst.append(scale_image(img3[ii])) #CT+Dose\n",
    "        \n",
    "        lst_trans = np.array(lst).transpose()\n",
    "        full_list.append(lst_trans)\n",
    "\n",
    "    full_array = np.array(full_list)\n",
    "    \n",
    "    return full_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SK6HOBtEX_Va"
   },
   "outputs": [],
   "source": [
    "# Define methods to import files, and perform splits to the data.\n",
    "\n",
    "# New method will import from zip and append.\n",
    "# It will return the appended array and the number of files (slices) in the dataset.\n",
    "def load_all_images(path):\n",
    "\n",
    "    # Extract all files to new directory with same name.\n",
    "    with zipfile.ZipFile(f\"{path}.zip\",\"r\") as zf:\n",
    "        zf.extractall(path)\n",
    "\n",
    "    # Loop through files in directory.\n",
    "    for ii, file in enumerate(os.listdir(path)):\n",
    "      data = np.load(path + '/' + file)\n",
    "      if ii == 0 :\n",
    "        concatData = np.load(path + '/' + file)\n",
    "      else :\n",
    "        data = np.load(path + '/' + file)\n",
    "        concatData = np.concatenate([concatData, data])\n",
    "\n",
    "    return concatData, ii + 1\n",
    "\n",
    "\n",
    "# Changed this method to take in the array, number of slices and indices.\n",
    "# The numslices will create an appropriate array \n",
    "def preset_split(X, train, validation, test):\n",
    "        \n",
    "    Xtrain = X[train]\n",
    "    Xval = X[validation]\n",
    "    Xtest = X[test]\n",
    "\n",
    "    return Xtrain, Xval, Xtest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGPUIRzTJqOt"
   },
   "source": [
    "## 1. Set up your data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3Hh0tuua0Ck"
   },
   "source": [
    "### Set Random Seed and Global Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0pDTnNca1kb"
   },
   "outputs": [],
   "source": [
    "# changed these to capital to be pythonic since these are constants\n",
    "\n",
    "# global seed: Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds.\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# operational seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "IMG_HEIGHT = 300 \n",
    "IMG_WIDTH = 300\n",
    "\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThzEec8OCsIy"
   },
   "source": [
    "###Import CT and Dose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6YCawaRGA20"
   },
   "outputs": [],
   "source": [
    "wd = '/home/dowen.paetkau/'\n",
    "wd = '/'\n",
    "\n",
    "# extract all pretreatment factors and labels\n",
    "\n",
    "# this assumes your file strucutre is labels_and_preconditions.zip/(all your other individual files).zip\n",
    "label_file_location = wd + 'content/labels_and_preconditions'\n",
    "unzip_data(f'{label_file_location}.zip')\n",
    "\n",
    "# this looking in the initally extracted folder and extracts all other zip files within this folder\n",
    "# the extracted files go to /content/\n",
    "for file in os.listdir(label_file_location):\n",
    "  unzip_data(label_file_location + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8-o_ivtQeCc"
   },
   "outputs": [],
   "source": [
    "wd = '/home/dowen.paetkau/'\n",
    "wd = '/'\n",
    "\n",
    "\n",
    "# To load images, have each of dose and ct files in zip files.\n",
    "# There can be as many slices as you like in the zip files, BE CONSISTENT.\n",
    "\n",
    "# When combining channels, be sure to rescale dose and ct before adding.\n",
    "# Otherwise the CT image will overpower dose image.\n",
    "\n",
    "# The np.tile function repeats the arrays after each other the specified times.\n",
    "\n",
    "# Load axial information.\n",
    "ax_ct, ii = load_all_images(wd + 'content/ct_axial')\n",
    "ax_dose, jj = load_all_images(wd + 'content/dose_axial')\n",
    "\n",
    "ax = combine_channels(ax_ct, ax_dose, scale_image(ax_ct) + scale_image(ax_dose))\n",
    "\n",
    "# Load sagittal information.\n",
    "sag_ct, ii = load_all_images(wd + 'content/ct_sagittal')\n",
    "sag_dose, jj = load_all_images(wd + 'content/dose_sagittal')\n",
    "\n",
    "sag = combine_channels(sag_ct, sag_dose, scale_image(sag_ct) + scale_image(sag_dose))\n",
    "\n",
    "# Load coronal information.\n",
    "cor_ct, ii = load_all_images('/content/ct_coronal')\n",
    "cor_dose, jj = load_all_images('/content/dose_coronal')\n",
    "\n",
    "cor = combine_channels(cor_ct, cor_dose, scale_image(cor_ct) + scale_image(cor_dose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YtY-ykqgcxY"
   },
   "outputs": [],
   "source": [
    "# Load and extend the patient labels.\n",
    "Y = np.tile(np.load(wd + 'content/mdadi_labels_binary_oh.npy'), ii)\n",
    "#Y = np.tile(np.load(wd + 'content/mdadi_labels_oh.npy'), ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLBpwFPUOMG9"
   },
   "outputs": [],
   "source": [
    "# Load pre-defined indices for train, validation and test sets.\n",
    "train_set = np.tile(np.load(wd + 'content/training_set.npy'), ii)\n",
    "val_set = np.tile(np.load(wd + 'content/validation_set.npy'), ii)\n",
    "test_set = np.tile(np.load(wd + 'content/test_set.npy'), ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuh6M87AoMxt"
   },
   "source": [
    "### Import pre-treatment factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHRu77VuoMxt",
    "outputId": "dd168d68-edae-44a9-8286-8e2037495217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(665, 18)\n"
     ]
    }
   ],
   "source": [
    "# Possibly cleaner way to deal with pre-treatment features\n",
    "site = np.tile(np.load(wd + 'content/cancer_site.npy', allow_pickle = True), ii)\n",
    "alcohol = np.tile(np.load(wd + 'content/alcohol_intake.npy', allow_pickle = True), ii)\n",
    "smoking = np.tile(np.load(wd + 'content/smoking_history.npy', allow_pickle = True), ii)\n",
    "n_stage = np.tile(np.load(wd + 'content/n_stage.npy', allow_pickle = True), ii)\n",
    "t_stage = np.tile(np.load(wd + 'content/t_stage.npy', allow_pickle = True), ii)\n",
    "\n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "site = onehotencoder.fit_transform(site.reshape(-1, 1)).toarray()\n",
    "alcohol = onehotencoder.fit_transform(alcohol.reshape(-1,1)).toarray()\n",
    "smoking = onehotencoder.fit_transform(smoking.reshape(-1, 1)).toarray()\n",
    "n_stage = onehotencoder.fit_transform(n_stage.reshape(-1, 1)).toarray()\n",
    "t_stage = onehotencoder.fit_transform(t_stage.reshape(-1, 1)).toarray()\n",
    "\n",
    "pretreatment_encoded = np.concatenate((site, alcohol, smoking, n_stage, t_stage), axis = 1)\n",
    "print(pretreatment_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6CXf4xCDAYe"
   },
   "source": [
    "### Create Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwT2NsZTJqOu",
    "outputId": "6dad603d-b670-45d9-d6a6-4315396854a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "\n",
      "Pre-treatment dataset\n",
      "(460, 18) (100, 18) (100, 18) \n",
      "\n",
      "Training Set:\n",
      "Shape - (460, 300, 300, 3)\n",
      "Class Split - [210 250]\n",
      "\n",
      "Validation Set:\n",
      "Shape - (100, 300, 300, 3)\n",
      "Class Split - [45 55]\n",
      "\n",
      "Testing Set:\n",
      "Shape - (100, 300, 300, 3)\n",
      "Class Split - [45 55]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_class = int(np.unique(Y).shape[0])\n",
    "print(f'Number of classes: {num_class}')\n",
    "\n",
    "# IMPLEMENT BALANCED SPLIT AND SHUFFLE\n",
    "Ytrain, Yval, Ytest = preset_split(Y, train_set, val_set, test_set)\n",
    "\n",
    "sag_train, sag_val, sag_test = preset_split(sag, train_set, val_set, test_set)\n",
    "cor_train, cor_val, cor_test = preset_split(cor, train_set, val_set, test_set)\n",
    "ax_train, ax_val, ax_test = preset_split(ax, train_set, val_set, test_set)\n",
    "\n",
    "pretreatment_train, pretreatment_val, pretreatment_test = preset_split(pretreatment_encoded, train_set, val_set, test_set)\n",
    "\n",
    "print(f\"\\nPre-treatment dataset\")\n",
    "print(pretreatment_train.shape, pretreatment_val.shape, pretreatment_test.shape, '\\n')\n",
    "\n",
    "print(f'Training Set:')\n",
    "print(f'Shape - {cor_train.shape}')\n",
    "print(f'Class Split - {np.bincount(Ytrain)}\\n')\n",
    "\n",
    "print(f'Validation Set:')\n",
    "print(f'Shape - {cor_val.shape}')\n",
    "print(f'Class Split - {np.bincount(Yval)}\\n')\n",
    "\n",
    "print(f'Testing Set:')\n",
    "print(f'Shape - {cor_test.shape}')\n",
    "print(f'Class Split - {np.bincount(Ytest)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6s9E_HyJqOv"
   },
   "source": [
    "## 2. Define your callbacks (save your model, patience, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRAhZsTeM9Hv"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Early Stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%10 == 0 and epoch!= 0:\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "# Learning Rate\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)\n",
    "\n",
    "# Tensorboard log\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback\n",
    "\n",
    "\n",
    "# save model\n",
    "def create_model_checkpoint(dir_name, experiment_name):\n",
    "  model_path = dir_name + '/' + experiment_name + '.h5'\n",
    "\n",
    "  # Create a ModelCheckpoint callback that saves the model\n",
    "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
    "                                                          monitor='val_loss',\n",
    "                                                          save_weights_only=False,\n",
    "                                                          save_best_only=True,\n",
    "                                                          verbose=1,\n",
    "                                                          mode='min')\n",
    "  return checkpoint_callback\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%10 == 0 and epoch!= 0:\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8xB10zuPOh6"
   },
   "source": [
    "## Define Data Augmentation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icw8E2s4PaRq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "data_augmentation = keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(IMG_HEIGHT,\n",
    "                                  IMG_WIDTH,\n",
    "                                  3)),\n",
    "  layers.RandomRotation(0.05),\n",
    "  layers.RandomTranslation(height_factor = (-0.1, 0.1), width_factor = (-0.1, 0.1), fill_mode = 'constant'),\n",
    "  # layers.RandomZoom(0.1),\n",
    "\n",
    "], name =\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLeY0X_Ewa1C"
   },
   "source": [
    "## Create a Baseline Model with AutoML: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR1gMJU4PN3-"
   },
   "outputs": [],
   "source": [
    "pip install autokeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XGLDX3owrSM"
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import tensorflow as tf\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM8JdKq6xCbT"
   },
   "outputs": [],
   "source": [
    "# DEFINE INPUTS \n",
    "sag_input_node = ak.ImageInput()\n",
    "cor_input_node = ak.ImageInput()\n",
    "ax_input_node = ak.ImageInput()\n",
    "\n",
    "# NORMALIZATION\n",
    "sag_output_node = ak.Normalization()(sag_input_node)\n",
    "cor_output_node = ak.Normalization()(cor_input_node)\n",
    "ax_output_node = ak.Normalization()(ax_input_node)\n",
    "\n",
    "#CONVOLUTIONS\n",
    "sag_output_node_2 = ak.ConvBlock()(sag_output_node)\n",
    "cor_output_node_2 = ak.ConvBlock()(cor_output_node)\n",
    "ax_output_node_2 = ak.ConvBlock()(ax_output_node)\n",
    "\n",
    "#MERGE\n",
    "output_node_concat = ak.Merge(merge_type='concatenate')([sag_output_node_2, cor_output_node_2, ax_output_node_2])\n",
    "output_node = ak.ClassificationHead(num_classes=2)(output_node_concat)\n",
    "\n",
    "auto_model = ak.AutoModel(\n",
    "    inputs = [sag_input_node, cor_input_node, ax_input_node], outputs = output_node, overwrite=True, max_trials=100\n",
    ")\n",
    "\n",
    "# Search\n",
    "auto_model_history = auto_model.fit(x=[sag_train, cor_train, ax_train], y=Ytrain, validation_data = ([sag_val, cor_val, ax_val],Yval))\n",
    "\n",
    "# Export as a Keras Model\n",
    "auto_model_ex = auto_model.export_model()\n",
    "print(type(auto_model_ex.summary()))\n",
    "\n",
    "# print model as image\n",
    "tf.keras.utils.plot_model(\n",
    "    auto_model_ex, show_shapes=True, expand_nested=True, to_file=\"auto_model.png\"\n",
    ")\n",
    "\n",
    "#Save results\n",
    "auto_model_ex.save('/content/saved_models/auto_model', save_format='h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYgpKw4zJqOw"
   },
   "source": [
    "## 3. Transfer Learning\n",
    "\n",
    "3.1 Choose and load your pretrained model without the top (i.e., the prediction part, usually the fully connected layers)\n",
    "\n",
    "3.2. Freeze the layers (i.e., make them non-trainable) of your pretrained model\n",
    "\n",
    "3.3. Add a top (i.e., the prediction layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFKiOUAxHLfo"
   },
   "source": [
    "### Create a model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzIutGQnZGkK"
   },
   "outputs": [],
   "source": [
    "def augment_data(input, flip=None, rotation=None, height_factor=None, width_factor=None):\n",
    "  if flip is not None:\n",
    "    aug = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=RANDOM_SEED)(input)\n",
    "  else:\n",
    "    aug = input\n",
    "  \n",
    "  if rotation is not None:\n",
    "    aug2 = tf.keras.layers.RandomRotation(rotation)(aug)\n",
    "  else:\n",
    "    aug2 = aug\n",
    "  \n",
    "  if height_factor is not None or width_factor is not None:\n",
    "    height_factor = 0 if height_factor is None else height_factor\n",
    "    width_factor = 0 if width_factor is None else width_factor\n",
    "  \n",
    "    aug3 = tf.keras.layers.RandomTranslation(height_factor = (-1 * height_factor, height_factor), width_factor = (-1*width_factor, width_factor), fill_mode = 'constant')(aug2)\n",
    "  else:\n",
    "    aug3 = aug2\n",
    "  \n",
    "  return aug3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZyCK5NoiWzt"
   },
   "outputs": [],
   "source": [
    "def flatten_layer(input, pooling):\n",
    "  if pooling == 'max':\n",
    "    return tf.keras.layers.GlobalMaxPool2D()(input)\n",
    "  elif pooling == 'average':\n",
    "    return tf.keras.layers.GlobalAveragePooling2D()(input)\n",
    "  elif pooling is None:\n",
    "    return tf.keras.layers.Flatten()(input)\n",
    "  else:\n",
    "    raise ValueError(\"value is optional, None, 'max' or 'average'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgeVR_dbTcDp"
   },
   "outputs": [],
   "source": [
    "# make a function to take in different models \n",
    "\n",
    "def make_model(keras_model, pooling=None, flip=None, rotation=None, height_factor=None, width_factor=None):\n",
    "  \"\"\"\n",
    "  keras_model: pre defined model to be passed in. Include if the model is trainable or not\n",
    "  pooling: either 'max', 'average' or None. \n",
    "    'max' means GlobalMaxPool2D is applied\n",
    "    'average' means GlobalMaxPool2D is applied\n",
    "     None means only Flatten is applied\n",
    "\n",
    "  rotation is directly related to RandomFlip. Can be either horizontal, vertical, horizontal_and_vertical, or None`\n",
    "\n",
    "  height_factor is directly related to the height_factor in tf.layers.RandomTranslation\n",
    "  width_factor is directly related to the width_factor in tf.layers.RandomTranslation\n",
    "    \n",
    "  \"\"\"\n",
    "\n",
    "  keras_model.trainable = False\n",
    "\n",
    "  # DEFINE INPUTS:\n",
    "  sag_input = tf.keras.layers.Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "  cor_input = tf.keras.layers.Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "  ax_input = tf.keras.layers.Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "  # DATA AUGMENTATION STEP\n",
    "  sag_aug = augment_data(input=sag_input, flip=flip, rotation=rotation, height_factor=height_factor, width_factor=width_factor)\n",
    "  cor_aug = augment_data(input=cor_input, flip=flip, rotation=rotation, height_factor=height_factor, width_factor=width_factor)\n",
    "  ax_aug = augment_data(input=ax_input, flip=flip, rotation=rotation, height_factor=height_factor, width_factor=width_factor)\n",
    "\n",
    " \n",
    "  # FLATTEN\n",
    "  sag_x = keras_model(sag_aug, training = False)\n",
    "  sag_flat = flatten_layer(sag_x, pooling=pooling)\n",
    "\n",
    "  cor_x = keras_model(cor_aug, training = False)\n",
    "  cor_flat = flatten_layer(cor_x, pooling=pooling)\n",
    "\n",
    "  ax_x = keras_model(ax_aug, training = False)\n",
    "  ax_flat = flatten_layer(ax_x, pooling=pooling)\n",
    "\n",
    "\n",
    "  # combine our 3 imagenet models\n",
    "  concatenated_images = tf.keras.layers.Concatenate(axis = 1)([sag_flat, cor_flat, ax_flat])\n",
    "\n",
    "\n",
    "  # pre-treatment layer\n",
    "  pretreatment_layer = tf.keras.layers.Input(shape = (pretreatment_train.shape[1],))\n",
    "  pretreatment_flattened = tf.keras.layers.Flatten()(pretreatment_layer)\n",
    "\n",
    "  # combine the pre-treatment layer with the concatenated imagenet layers\n",
    "  images_pretreatment_combined = tf.keras.layers.Concatenate(axis=1)([concatenated_images, pretreatment_flattened])\n",
    "\n",
    "  # Combine the final model.\n",
    "  out = tf.keras.layers.Dense(num_class,activation = 'softmax')(images_pretreatment_combined)\n",
    "  model = tf.keras.Model(inputs = [sag_input, cor_input, ax_input,\n",
    "                                  pretreatment_layer], outputs = out)\n",
    "\n",
    "  print(\"Initial Training Model\")\n",
    "  print(model.summary())\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJA26YE08upf"
   },
   "source": [
    "## 3.4 Train and Fit you dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvwsADSN8t9c"
   },
   "outputs": [],
   "source": [
    "def train_and_fit(model, model_name):\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  model_history = model.fit([sag_train, cor_train, ax_train, pretreatment_train], \\\n",
    "                        Ytrain, epochs = 25, verbose = 1, \\\n",
    "                        callbacks= [early_stop, \n",
    "                                    create_model_checkpoint('saved_models', model_name), \n",
    "                                    lr_schedule, \n",
    "                                    create_tensorboard_callback('transfer_learning', model_name)], \\\n",
    "                                    validation_data = ([sag_val, cor_val, ax_val, pretreatment_val], Yval),\n",
    "                                    steps_per_epoch=len(sag_train),\n",
    "                                    validation_steps=len(sag_val), \n",
    "                                    )\n",
    "  return model_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONAOCkrI8zh8"
   },
   "source": [
    "## Combining making a model and training into one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA37Gj3X86rU"
   },
   "outputs": [],
   "source": [
    "def make_and_train_model(model, model_name,  pooling=None, flip=None, rotation=None, height_factor=None, width_factor=None):\n",
    "  tl_model = make_model(model, pooling=pooling, flip=flip, rotation=rotation, height_factor=height_factor, width_factor=width_factor)\n",
    "  model_history = train_and_fit(tl_model, model_name)\n",
    "  plot_loss_curves(model_history)\n",
    "  return model_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYqC3PwUFyoJ"
   },
   "source": [
    "## Define Models to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcsrloTMDizw"
   },
   "outputs": [],
   "source": [
    "## Define Models we to use\n",
    "\n",
    "models = {'b3_base_it':tf.keras.applications.EfficientNetB3(weights='imagenet', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdwHzaCPF6ax"
   },
   "source": [
    "## Loop Through different iterations of augmentation and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZRQoM5MF328"
   },
   "outputs": [],
   "source": [
    "# these are the different configurations to test against\n",
    "pooling = [None, 'max', 'average']\n",
    "flip_aug = [None, 'horizontal_and_vertical']\n",
    "rotation_aug = [None, 0.1, 0.2, 0.5]\n",
    "translation_aug = [None, 0.1, 0.2, 0.4]\n",
    "\n",
    "def iterate_models(model_name, model):\n",
    "  for pool in pooling:\n",
    "    pool_name = f'{model_name}_pooling_{str(pool)}'\n",
    "    for flip in flip_aug:\n",
    "      flip_name = pool_name + f'_flip_{flip}'\n",
    "      for rotation in rotation_aug:\n",
    "        rotation_name = flip_name + f'_rotation_{rotation}'\n",
    "        for translation in translation_aug:\n",
    "          final_model_name = rotation_name + f'_translation_{translation}'\n",
    "          # actually make the models\n",
    "          make_and_train_model(model, final_model_name, pooling=pooling, flip=flip, rotation=rotation, height_factor=translation, width_factor=translation)\n",
    "          # reset the name in this layer\n",
    "          final_model_name = rotation_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdT-Nb1XHDfb"
   },
   "source": [
    "## Build a models via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGKSfcRCyrsX"
   },
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "  iterate_models(model_name=model_name, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG7wrvtMFroU"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldG6QsEuxg3Y"
   },
   "source": [
    "### Import I.T. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXgb8_A2xkP6"
   },
   "outputs": [],
   "source": [
    "model_loc = wd + \"content/\"\n",
    "\n",
    "model_it_1_name = \"b3_base_it_pooling_average_flip_horizontal_and_vertical_rotation_0.5_translation_0.4.h5\"\n",
    "model_it_2_name = \"b3_base_it_pooling_None_flip_None_rotation_0.1_translation_0.4.h5\"\n",
    "model_it_3_name = \"b3_base_it_pooling_None_flip_horizontal_and_vertical_rotation_None_translation_0.2.h5\"\n",
    "model_it_4_name = \"b3_base_it_pooling_None_flip_horizontal_and_vertical_rotation_0.5_translation_0.2.h5\"\n",
    "model_it_5_name = \"b3_base_it_pooling_max_flip_horizontal_and_vertical_rotation_0.1_translation_0.1.h5\"\n",
    "model_it_6_name = \"b3_base_it_pooling_average_flip_None_rotation_None_translation_0.4.h5\"\n",
    "model_it_7_name = \"b3_base_it_pooling_average_flip_horizontal_and_vertical_rotation_None_translation_0.4.h5\"\n",
    "model_it_8_name = \"b3_base_it_pooling_average_flip_horizontal_and_vertical_rotation_0.1_translation_0.1.h5\"\n",
    "model_it_9_name = \"b3_base_it_pooling_None_flip_None_rotation_0.1_translation_0.1.h5\"\n",
    "model_it_10_name = \"b3_base_it_pooling_None_flip_horizontal_and_vertical_rotation_0.2_translation_0.4.h5\"\n",
    "\n",
    "it_model_list = [model_it_1_name, model_it_2_name, model_it_3_name, model_it_4_name, model_it_5_name, model_it_6_name, model_it_7_name, model_it_8_name, model_it_9_name, model_it_10_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45tLczzUxnGL"
   },
   "source": [
    "Fine Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pantUD1_xouA"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB4wm0xFxpe_"
   },
   "outputs": [],
   "source": [
    "def train_and_fit_fine_tune(model, model_name):\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-7),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  model_history = model.fit([sag_train, cor_train, ax_train, pretreatment_train], \\\n",
    "                        Ytrain, epochs = 25, verbose = 1, \\\n",
    "                        callbacks= [early_stop, create_model_checkpoint('/content/ft_models', model_name), lr_schedule], \\\n",
    "                        validation_data = ([sag_val, cor_val, ax_val, pretreatment_val], Yval), \\\n",
    "                        batch_size = BATCH_SIZE \n",
    "                        )\n",
    "  return model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHM-YteKf22Z",
    "outputId": "1e370c02-61d6-4361-b236-41bae41c8329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "39/39 [==============================] - 97s 2s/step - loss: 0.4411 - accuracy: 0.8696 - val_loss: 0.5670 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 2/20\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.4353 - accuracy: 0.8674 - val_loss: 0.5663 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 3/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4338 - accuracy: 0.8587 - val_loss: 0.5658 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 4/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4394 - accuracy: 0.8609 - val_loss: 0.5651 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 5/20\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.4379 - accuracy: 0.8543 - val_loss: 0.5646 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 6/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4340 - accuracy: 0.8783 - val_loss: 0.5640 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 7/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4279 - accuracy: 0.8783 - val_loss: 0.5630 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 8/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4396 - accuracy: 0.8565 - val_loss: 0.5627 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 9/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4226 - accuracy: 0.8848 - val_loss: 0.5621 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 10/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4293 - accuracy: 0.8761 - val_loss: 0.5616 - val_accuracy: 0.6500 - lr: 1.0000e-07\n",
      "Epoch 11/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4353 - accuracy: 0.8696 - val_loss: 0.5613 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 12/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4308 - accuracy: 0.8783 - val_loss: 0.5611 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 13/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4224 - accuracy: 0.8891 - val_loss: 0.5609 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 14/20\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.4283 - accuracy: 0.8804 - val_loss: 0.5607 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 15/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4148 - accuracy: 0.8804 - val_loss: 0.5603 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 16/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4272 - accuracy: 0.8739 - val_loss: 0.5600 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 17/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4309 - accuracy: 0.8783 - val_loss: 0.5598 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 18/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4362 - accuracy: 0.8674 - val_loss: 0.5596 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 19/20\n",
      "39/39 [==============================] - 57s 1s/step - loss: 0.4251 - accuracy: 0.8826 - val_loss: 0.5593 - val_accuracy: 0.6500 - lr: 5.0000e-08\n",
      "Epoch 20/20\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.4266 - accuracy: 0.8783 - val_loss: 0.5590 - val_accuracy: 0.6500 - lr: 5.0000e-08\n"
     ]
    }
   ],
   "source": [
    "def fine_tune_all_models(it_model_list):\n",
    "\n",
    "  model_loc = \"/content/\"\n",
    "\n",
    "  for it_model_name in it_model_list:\n",
    "    model = tf.keras.models.load_model(model_loc + it_model_name)\n",
    "    model.trainable = True\n",
    "    print(it_model_name)\n",
    "    print(model.summary())\n",
    "\n",
    "    model_history = train_and_fit_fine_tune(model, it_model_name.replace(\"_it_\", \"_ft_\"))\n",
    "\n",
    "    plot_loss_curves(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3hkUQfqxvHl"
   },
   "source": [
    "## Fine Tune All 10 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be_Ydiebxy8-"
   },
   "outputs": [],
   "source": [
    "fine_tune_all_models(it_model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zTq8He_x2as"
   },
   "source": [
    "# Test Fine Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMnHRoVJx794"
   },
   "source": [
    "### Define top 3 fine tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J0nXnwPx9-6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1_wy7rOx_De"
   },
   "outputs": [],
   "source": [
    "model_it_1_name = \"b3_base_it_pooling_None_flip_horizontal_and_vertical_rotation_0.5_translation_0.2\"\n",
    "model_ft_1_name = \"b3_base_ft_pooling_None_flip_horizontal_and_vertical_rotation_0.5_translation_0.2\"\n",
    "\n",
    "model_it_2_name = \"b3_base_it_pooling_average_flip_None_rotation_None_translation_0.4\"\n",
    "model_ft_2_name = \"b3_base_ft_pooling_average_flip_None_rotation_None_translation_0.4\"\n",
    "\n",
    "model_it_3_name = \"b3_base_it_pooling_average_flip_horizontal_and_vertical_rotation_0.5_translation_0.4\"\n",
    "model_ft_3_name = \"b3_base_ft_pooling_average_flip_horizontal_and_vertical_rotation_0.5_translation_0.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW5cAJquyGTU"
   },
   "source": [
    "### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dlg7GY7yGmf"
   },
   "outputs": [],
   "source": [
    "def test_models(model_it_name, model_ft_name):\n",
    "\n",
    "  print(f'Testing Model: {model_ft_name}:\\n')\n",
    "\n",
    "  print('Before Fine Tuning:')\n",
    "  model = tf.keras.models.load_model(\"/content/\" + model_it_name + \".h5\")\n",
    "  model.evaluate([sag_test, cor_test, ax_test, pretreatment_test],Ytest)\n",
    "\n",
    "  print('\\nAfter Fine Tuning: ')\n",
    "  model = tf.keras.models.load_model(\"/content/\" + model_ft_name + \".h5\")\n",
    "  model.evaluate([sag_test, cor_test, ax_test, pretreatment_test],Ytest)\n",
    "\n",
    "  Ypred = model.predict([sag_test, cor_test, ax_test, pretreatment_val]).argmax(axis = 1)\n",
    "  print(f'Test Set Truth: \\n{Ytest}\\n')\n",
    "  print(f'Test Set Prediction:\\n{Ypred}\\n')\n",
    "\n",
    "  \n",
    "  print('Confusion Matrix:')\n",
    "  cm=confusion_matrix(Ytest, Ypred)\n",
    "  print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yzmU7DVyIdo"
   },
   "source": [
    "### Test 3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNgcV8L2yKJP"
   },
   "outputs": [],
   "source": [
    "test_models(model_it_1_name, model_ft_1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a72CODljyL6m"
   },
   "outputs": [],
   "source": [
    "test_models(model_it_2_name, model_ft_2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHJvNShIyN6j"
   },
   "outputs": [],
   "source": [
    "test_models(model_it_3_name, model_ft_3_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2PMssDXyPW9"
   },
   "source": [
    "### Look into incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUsD37tDyPg3"
   },
   "outputs": [],
   "source": [
    "wrong_indexes = np.where(Ypred != Ytest)[0]\n",
    "\n",
    "# Disaplying some samples from the development set\n",
    "sample_indexes = np.random.choice(np.arange(wrong_indexes.shape[0], dtype = int),size = 8, replace = True)\n",
    "plt.figure(figsize = (24,18))\n",
    "for (ii,jj) in enumerate(sample_indexes):\n",
    "    plt.subplot(4,2,ii+1)\n",
    "    aux = sag_test[wrong_indexes[jj]]\n",
    "    aux = (aux - aux.min())/(aux.max() - aux.min())\n",
    "    plt.imshow(aux, cmap = \"gray\")\n",
    "    plt.title(\"Label: %d, predicted: %d\" %(Ytest[wrong_indexes[jj]],Ypred[wrong_indexes[jj]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcDhXc1ByVmx"
   },
   "source": [
    "### Look into correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThuY8_JyyZjT"
   },
   "outputs": [],
   "source": [
    "right_indexes = np.where(Ypred == Ytest)[0]\n",
    "\n",
    "# Disaplying some samples from the development set\n",
    "sample_indexes = np.random.choice(np.arange(right_indexes.shape[0], dtype = int),size = 8, replace = True)\n",
    "plt.figure(figsize = (24,18))\n",
    "for (ii,jj) in enumerate(sample_indexes):\n",
    "    plt.subplot(4,2,ii+1)\n",
    "    aux = sag_test[right_indexes[jj]]\n",
    "    aux = (aux - aux.min())/(aux.max() - aux.min())\n",
    "    plt.imshow(aux, cmap = \"gray\")\n",
    "    plt.title(\"Label: %d, predicted: %d\" %(Ytest[right_indexes[jj]],Ypred[right_indexes[jj]]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ENEL645_group5_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
